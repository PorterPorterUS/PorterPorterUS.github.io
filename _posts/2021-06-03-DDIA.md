---
layout: post
author: Xusheng Ji
title: "DDIA"
tags: System Design
categories: Database System
---

{% include lib/mathjax.html %}


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>



DDIA=
=====================Single leader replications ==========================
1.写入操作必须通过leader, 然后leader把写入结果传递给所有的follower
2. 读取操作可以通过任意节点，leader或者follower都可以

同步和非同步的follower：
同步的follower: leader发送给follower,需要等待follower的返回才能执行下一步操作
异步的follower：leader发送给follower，之后可以做任何的事情，不需要等待。
通常集群中只有一个同步的follower，为的是怕leader挂掉，follower可以重新担当leader的职责，其他的节点都是异步的follower，为的是可以提升性能。

新加入的节点追赶
首先对leader的数据进行snapshot，然后把snapshot传递给follower，完毕之后follower问leader索要这段时间内产生变化的数据


处理挂机问题：
如果follower挂掉了，重启之后他会查看自己的log处理到哪一步了，然后向leader索要挂机这段时间内产生的数据变化
如果leader挂机了，需要三步，第一步是集群通过timeout来确定leader挂机了，如果follower一段时间内没收到leader的message,那么证明leader挂机。第二步，选举新的leader，一般选取最up-to-date的follower 第三步，client需要把写操作传递给新的leader，一旦old leader恢复了，一定要确认它变为follower。

Replication Logs的实现方式
1.statement-based replication
log记录的是数据库中每条statement(insert,delete,select),但是问题在于，某些statement是non-deterministic的，在不同的时间，或者不同的情况下执行的结果是不一样的，例如 now()或者自增函数，所以单纯的记录statement是行不通的
2.WAL
WAL是很底层的log，描述的是硬盘中哪个block的哪个byte被修改了，如果leader可以把WAL传递给follower，那么follower可以获取相同的数据。问题在于，假如follower和leader使用的storage version不一样，那么对于相同的log会产生不同的结果。如果follower的版本more newer的话，那么可以使用failover的办法重新选举，让这个follower作为新的leader，
3.Logic replication
和WAL不同的是，可以对storage version进行解耦，对于insert,delete,updated有着业务逻辑上的描述。这就允许follower和leader有着不同的storage engine version。

Leader-based replication的问题
replication的作用：可以使用更多的节点处理更多的请求++让一些replicas更接近users++fault tolerance
问题：有一些异步的follower会处于outdated状态，读取这些follower会产生过期数据，另外如果是同步的follower，那么集群如果很大，等待所有follower进行同步非常耗时
解决方案：
1.read-your-write consistency: 如果读取的是用户自己修改的数据，那么就从leader读。如果application大部分都是用户的修改的数据，可以设置时间限制，超过一定的时间会产生lagging，所以就从leader读取，如果replicas不够up-to-date,则从其他follower读取或者等待catch-up之后再读取。
2.时间一致性->单调读consistency: 如果client从replica的位置读取了两次，第一次读的是稍微过期的数据，第二次读的是过期很大的数据，则会产生先看到有评论，第二次再读取发现没有评论。单调读确保这种事情绝不会发生。解决方案就是最好都从一个replica读取。
3.因果一致性，如果Write A写入原因，write B写入结果，如果一个client读取的时候A产生了一些lagging，导致client先看到结果后看到原因。因果一致性确保了 如果写入的时候有顺序，那么读取的时候一定要按照写入的顺序来读取。

最期待的结果是developer不需要担心一致性，一致性应该由数据库来实现，对于单节点系统来说，使用事务transaction是最好的方案，但是性能有问题，所以大家摒弃了single-leader 


========================multiple  leader replications =============================
multi-leaders replications的应用场景：
1.如果集群需要跨越多个data centers的话，应该在每一个data center都设置一个leader,这样可以提升性能++容错++减少网络传输
2.对于offline application, 例如日历，当你在offline的时候操作，当恢复online的时候，需要和services进行同步处理, 你的每一台设备相当于一个leader，那么你的多个设备就像一个multi-leader的data center一样，他们之间需要互相同步
3.对于共同编辑文档的应用，通常情况下来说可以堪称single-leader模式，一个用户在编辑，另外一个用户必须等候解锁，但是为了加速操作，必须允许多个用户共同操作，这就是multi-leaders的模式


multi-leaders处理写入冲突：
在多人共同编辑文档应用中, 如果是single-leader replication,那么第一个writer写入的时候，第二个writer应该被block住，直到第一个写完之后它才能进行写入。
但是在multi-leaders中, 两个writers可以同时写入自己datacenter中的leader，然后在两个leaders进行同步的时候，发现conflict.
解决方案：
1.修改同一个field都转移到相同的leader上
2.给每一个写入操作赋予一个ID，ID大的覆盖掉ID小的写入操作
3.关于multi-leaders的paper，``Conflict-free replicated datatypes (CRDTs)`` ``Mergeable persistent data structure`` ``Operational transformation ``


Multi-leaders replication Topology:
因为多个leaders之间要进行同步策略,那么leaders之间的数据流向是怎么样的呢？
1.Circular
2.Star
3.All-to-all
对于前两种的问题在于，fault-tolerance处理的不好，一个节点挂掉了，其他的节点无法传递信息。每个node都有自己的ID，每次message被一个node接收之后，这条消息就要加上NODEID，直到消息上所有的NODEID都被标注之后，才证明传递完毕
All-to-all的问题在于，某个node收到其余两个node传递来的消息顺序和真实的逻辑顺序不一致，例如先收到结果，再收到原因，这会违反 因果一致性。



=============================leaderless replications ==================================
1.Quorum
对于没有leader的cluster,所有写入操作都要经过所有的nodes,如果收到 (N/2)+1 = w 以上node的回复，就算是写入成功
所有读取操作都要经过所有nodes, 读要读取r个节点, 因为这R个节点中肯定有确保里面有up-to-date的node
N=all nodes, w=r=(N+1)/2(round up), N一般是奇数, 但是如果是读多写少的系统，可以设置w=n,r=1即可

把w,r设置的比较小，虽然会读取stale data, 但是在很多机器挂掉不能使用的情况下，可以提供HA+lower latency

如果w,r 满足了w+r>n, 这种情况下，仍然会导致读到stale data.
1.Sloppy quorum: r,w不再有overlap
2. 多个读写并发产生，如果last winner wins, writes会丢失。
3. 读和写并发产生， 写入有可能没有写完w个节点,读取就开始发生了,所以会读取stale data.
4. 如果读取发生在w个节点上，但是不是所有的节点都写入成功，那么后续的read操作有可能读到最新数据，也可能读不到最新数据
虽然Quorum理论上读取可以读到最新数据，但是实际上会有很多问题。

对于leader-based replication来说，可以很好的monitoring，因为只要计算出当前stale日志的ID和leader日志的ID之间的差距，即可知道落后了多少。


2.Sloppy Quorums and Hinted Handoff
因为每次读写只需要等待w/r个节点, 所以Quorums有一定的容错性，但是网络出现问题，client无法连接w/r个节点，于是无法凑成w+r>n的条件。
这个时候如果开启了sloppy quorums功能可以暂时使用n之外的节点暂时接受读写请求，等到网络恢复再把读写请求转为正常的n个节点(hinted handoff)
所以sloppy quorums仍然要满足w+r>n.


3.处理并发写入
leaderless replication允许同一个client对同一个key进行多次写入，但是不同的node看到的结果顺序很可能不同，节点1看到先A后B，节点2看到先B后A，所以会出现不一致现象。
解决方案：
3.1 last write wins(LWW)
对每个写入操作都加入一个timestamp, 不同的node只以最后一个timestamp为主, 会把前面的数据给覆盖掉，保证数据一致性。Cassandra就是这么做的，但是问题在于，假如我们想保留之前的数据，LWW会把之前的数据覆盖。解决方案是可以使用UUID来做。

4.怎么定义并发写入？？
和时间没有关系，只要两个写入操作有逻辑上的依赖(A:insert where x=1 B:update x to 1)，就必然不是并发写入.
如果没有逻辑上的依赖，可以定义为并发。
1. Client 1 adds milk to the cart. This is the first write to that key, so the server suc‐ cessfully stores it and assigns it version 1. The server also echoes the value back to the client, along with the version number.
2. Client 2 adds eggs to the cart, not knowing that client 1 concurrently added milk (client 2 thought that its eggs were the only item in the cart). The server assigns version 2 to this write, and stores eggs and milk as two separate values. It then returns both values to the client, along with the version number of 2.
3. Client 1, oblivious to client 2’s write, wants to add flour to the cart, so it thinks the current cart contents should be [milk, flour]. It sends this value to the server, along with the version number 1 that the server gave client 1 previously. The server can tell from the version number that the write of [milk, flour] supersedes the prior value of [milk] but that it is concurrent with [eggs]. Thus, the server assigns version 3 to [milk, flour], overwrites the version 1 value [milk], but keeps the version 2 value [eggs] and returns both remaining values to the client.
4. Meanwhile, client 2 wants to add ham to the cart, unaware that client 1 just added flour. Client 2 received the two values [milk] and [eggs] from the server in the last response, so the client now merges those values and adds ham to form a new value, [eggs, milk, ham]. It sends that value to the server, along with the previ‐ ous version number 2. The server detects that version 2 overwrites [eggs] but is concurrent with [milk, flour], so the two remaining values are [milk, flour] with version 3, and [eggs, milk, ham] with version 4.
5. Finally, client 1 wants to add bacon. It previously received [milk, flour] and [eggs] from the server at version 3, so it merges those, adds bacon, and sends the final value [milk, flour, eggs, bacon] to the server, along with the version number 3. This overwrites [milk, flour] (note that [eggs] was already over‐ written in the last step) but is concurrent with [eggs, milk, ham], so the server keeps those two concurrent values.

<img width="1065" alt="截屏2021-05-30 下午12 24 02" src="https://user-images.githubusercontent.com/60555283/120111979-f1e0ab00-c141-11eb-8a89-ba2215250104.png">



4.1 The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written.
4.2 When a client reads a key, the server returns all values that have not been over‐ written, as well as the latest version number. A client must read a key before writing.
4.3 When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. (The response from a write request can be like a read, returning all current values, which allows us to chain several writes like in the shopping cart example.)
4.4 When the server receives a write with a particular version number, it can over‐ write all values with that version number or below (since it knows that they have been merged into the new value), but it must keep all values with a higher ver‐ sion number (because those values are concurrent with the incoming write).

最后三种一致性model

1.Read-after-write consistency
Users should always see data that they submitted themselves.
2. Monotonic reads
After users have seen the data at one point in time, they shouldn’t later see the data from some earlier point in time.
3. Consistent prefix reads
Users should see the data in a state that makes causal sense: for example, seeing a question and its reply in the correct order.



==========================PARTITION====================


===================================Fault tolerance===================================
要实现Fault tolerance必须实现一种抽象，例如在事务性数据库中，事务可以保证实现ACID，application开发者可以不用考虑这些问题。
对于Fault tolerance的抽象就是共识，比如leader挂了，所有的follower都达成一种共识，也就是需要重新选举新的节点。
所以我们的重点就是讨论如何实现共识机制。

strong consistency通常以牺牲性能和fault-tolerance为代价。
consistency的级别经常和transaction isolation的level相对应。但是事务主要关注的是并发操作中有没有race condition，而consistency关注的是协调状态的一致性。

1.linearizability(strong,atomic,external).
一个用户写完之后，任何的读取都必须要读取到最新鲜的数据，不能有任何stale data产生。虽然分布式系统中有很多节点，但是任何读写都要表现的像single node一样，任何操作都是atomic的

截屏2021-05-30 下午6.41.22![image](https://user-images.githubusercontent.com/60555283/120122438-a8f71980-c176-11eb-9584-7f7f5e34e736.png)

写入操作什么时候发生无所谓，但是一旦有一个read操作读取到了最新的数据，那么后续的 read操作一定不能读取到stale data.

2.Linearizaibility和 Serializability的区别：
后者是ACID 事务性数据库中保证的内容，并发执行的事务一定要表现的像一个有顺序的transaction execution order一样，即使这个order和真实的执行顺序不一样
前者在乎的是数据的新鲜度，每次写入完成之后，后续的读取操作一定要读到最新鲜的数据。
数据库中经常实现Linearizaibility+Serializability， 办法是2PL or  actual serial execution.
但是snapshot isolation并不是linearizable


3.Linearizability的使用场景
3.1 Lock和leader选举，通常情况下，ZooKeeper/etcd 这种协调性的服务都是使用分布式锁来实现Linearizability，一旦一个node获取了lock，另外其他的所有节点都必须用共识的办法承认这个node获取了leader。
3.2 唯一性的保证，比如创建linux path 或者 邮箱地址，一个用户一旦获取了锁创建了这个名字，其他用户不能再次创建相同的内容。
3.3.跨通道的时间保证，web server会先把图片存储到file storage中，然后通过MQ向file storage发送 resize的指令，假如指令发送的太快，比file storage中图片产生的变化更快，那么用户会看到stale data.所以这里需要Linearizability。


截屏2021-05-30 下午6.54.46![image](https://user-images.githubusercontent.com/60555283/120122736-9d0c5700-c178-11eb-8dcd-8364ad27637c.png)

4.实现Linearizability
实现fault-tolerance最好的办法是通过replication
4.1 对于single-leader replication：
通过读取leader或者sync的follower，一般来说会达成linearizability,但是如果leader是假的(拜占庭问题),那么就会破坏 linearizability。
共识算法解决了stale data和脑裂现象，所以共识算法会实现linearizability.

4.2 对于multiple-leader replication：
无法保证Linearizability

4.3 对于leaderless replication:
quorums并不一定会实现Linearizability，因为Last write wins是基于时钟系统，时钟系统无法正保证是真是的ordering。

截屏2021-05-30 下午7.12.36![image](https://user-images.githubusercontent.com/60555283/120123116-04c3a180-c17b-11eb-839c-e218998b5c00.png)



B的读取发生在A写入之后，但是无法保证读取到新鲜数据，因为B读取两个replica并没有被A写入完成。所以我们说leaderless replication就算实现了quorum，一样无法达成Linearizability.
除非：a reader must perform read repair synchronously, before returning results to the application。 
and a writer must read the latest state of a quorum of nodes before sending its writes。


5.cost of Linearizability
在multi-leaders replications的情况下，不同leader所领导的data center之间出现了网络问题，互相无法通信。此时此刻，系统应该允许读写操作可以继续呢(保证可用性，但是会读取到过期数据)？还是不让读写操作继续(保证Linearizability，但是无法保证可用性)？
这就是CAP理论所涉及的问题，当网络很好的情况下，我们可以既选择HA又选择consistency，但是网络不好的情况下必须二者选其一。也就是CAP理论，要不就是CP or AP.
当代计算机系统中的多核CPU也不是Linearizability的，每一个core都有自己的buffer和cache,计算机这么设计是为了达成性能上的优势。


1.因果一致性
因果一致性是partial order, Linearizability是total order。也就是说，在Linearizability中的任何的操作都能拿到对应的顺序，但是因果一致性只是保证业务逻辑上的前后顺序，假如不存在数据依赖关系，那么什么顺序都无所谓。
在强一致性的系统中，没有并发操作。对于所有的operations一定有唯一的顺序保证。
大部分的数据系统都使用因果一致性而不是强一致性，因为考虑到性能问题。强一致性一般也就顺便实现了因果一致性
snapshot isolation 提供的就是因果一致性


在实际工作中，追求因果关系很不现实，因为application在写入之前通常需要读取大量的数据，很难确定write operation取决于哪个read读取。
所以，我们可以使用sequence number 或者 timestamp(come from logical clock)，他们一般来说都是total order的, 每个操作都有唯一的ID，任意两个操作都可以被对比。
consistent with causality: 如果A happens before B，那么A sequence number一定在B之前。如果是并发的操作，无法保证sequence number是否递增以及有顺序。
在single-leader replication中, replication logs就定义了total order的写入操作并且consistent with causality. leader可以对每一个operation都设置一个递增的ID，如果follower使用这些ID就可以满足因果一致性。   
如果是multiple-leaders, 产生sequence number的办法是，(一个节点产生偶数，一个节点产生奇数)+++(每个操作附加一个timestamp)+++(每个节点提前allocation一定量的ID)
虽然这三种办法可以实现sequence number 但是无法consistent with causality，也就是说虽然是递增的ID，但是并不一定能保证业务逻辑上的顺序准确。

所以使用Lamport timestamp(LT), 可以保证了consistent with causality. The key idea about Lamport timestamps, which makes them consis‐ tent with causality, is the following: every node and every client keeps track of the maximum counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.

仅仅只有完美的timestamp 或者sequence number还不够， 时间戳只有把所有操作全部收集之后，才能有一个顺序。但是某个节点接受了创建文件的请求，他必须保证其他节点没有正在创建相同的文件名字并且以一个lower 时间戳在创建。所以必须去每个几点上去check。






















