---
layout: post
author: Xusheng Ji
title: "DDIA"
tags: System Design
categories: Database System
---

{% include lib/mathjax.html %}


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

==================================设计推特============================

1.先问哪些功能？ 推特有Tweet功能，也就是每个博主可以把自己的推特内容发送给所有的follower，T
imeline功能，你自己可以看自己的推特以及你关注的好友的推特并且还可以搜索。Trends功能，也就是热搜功能。
2.再问哪些性能？推特是read-heavy+eventually consistency.


1.首先考虑数据存储，为了快速读取数据，可以使用redis,同时为了让数据的持久化更加可靠，可以存入DB。格式如下：
截屏2021-06-02 下午7.04.44![image](https://user-images.githubusercontent.com/60555283/120562929-6bdc9280-c3d5-11eb-856c-0fdec1e7db98.png)


2. Home Timeline: 可以获取到每个following的latest tweets. 思路是每个following应该把自己的每一条tweet以fan out的方式发送给所有的followers。
首先每个博主把自己的tweet写入到DB, 然后写入自己的UT的cache, 写完毕之后把这条tweet以fan out的形式发送给自己的粉丝所有的cache.

非明星使用fan out 下午7.14.13![image](https://user-images.githubusercontent.com/60555283/120563500-bdd1e800-c3d6-11eb-9f0c-6a3fc67bf38f.png)



但是如果对明星来说，这种形式不行，不能使用fan out的形式。应该具有的形式应该是每个粉丝把自己所有明星全部拿出来放到另外一个cache中, 针对每个明星都去它对应的cache中最新的tweet,然后添加到自己的homeline tweet cache中。

明星不用fan out 下午7.23.50![image](https://user-images.githubusercontent.com/60555283/120564066-15bd1e80-c3d8-11eb-9f88-c79109a15b2d.png)


3.热搜：
首先使用kafka把每一条tweet都存进来，然后用apache storm取出来放入第一个operator filter, 这个operator的作用是检查这个tweet的hashtag是不是一些food,fun等等非流量hashtag,如果是，那么这个tweet一定不能作为热搜。第二个operator parse, 并不是每个tweet都有自己的hashtag,那么这个operator的作用就是为每个tweet添加自己的hashtag,并且删除stop words. 从这里开始，开始分为两条路，一条路是rank,计算当前hashtag有多少条tweet,并且对目前最火热的tweet hashtag进行排序。上面的路是计算发送这条tweet的地理位置，同时需要计算当前地理位置最火热的tweets hashtag.

使用Apache Storm来实现热搜功能![image](https://user-images.githubusercontent.com/60555283/120564188-62085e80-c3d8-11eb-8ce6-ccc05daed0ee.png)


4.搜索功能
使用倒排索引功能，对每个tweet分割成不同的字符，针对每个字符统计涉及这个字符都有哪些tweets, 当然这个倒排索引需要分布式的去存储，所以在读取的时候就是分布式的去读。

搜索功能![image](https://user-images.githubusercontent.com/60555283/120564809-b7913b00-c3d9-11eb-839f-b93273a7ebe9.png)


5.整体架构：
首先过了load balancer之后，会有三个组成部分，第一个tweets writer,每次生成一个新的tweet,都先把这个tweet发送给DB, 然后发送给apache storm做hashtag trending,同时以fan out的形式发送给每个粉丝到redis中，并且发送给search services作为倒排索引。 第二个是如果user调用home timeline功能，那么会直接调用redis,进行搜索所有博主的tweets。最终我们需要用ZK对redis进行coordinate功能，帮助选举，服务发现(哪个节点挂了，哪个节点上线了)

整体架构![image](https://user-images.githubusercontent.com/60555283/120565082-4ef68e00-c3da-11eb-9332-91f314274276.png)




==================================设计UBER========================================
1.首先对于数据存储，Uber用的google S2 lib, 把地图划分为多个block,每个block有自己的id,然后这些block分布式的存储在不同的节点之上，当你访问的具体的位置的时候,只需要找到对应block的server即可。当一个用户在发出打车请求的时候，uber使用的S2 lib可以把当前的block IDs 全部发送给Uber, 这样我们就可以计算距离。

2.WAF(web app firewall)的作用是安全因素,可以屏蔽各种危险的请求，并且不在uber主营业务区内的请求一样可以屏蔽。汽车平均每10秒种可以发送自己的地理位置给Kafka restful API,然后把数据push到Kafka.在有打车请求的时候，kafka会把最新的地理位置发给DISCO服务。Web Socket的作用要保证client/app与server之间保持连接状态, 是client和服务内部不停的有数据交互，所以使用web socket连续链接。

3.DISCO：每个region server都是很平均的存储了一些block id, 首先把所有的region servers 组成一个topology ring,然后外层是demand server和supply server. 对于scale来说，用了两个办法，一个是使用一致性hash,把任务平均的分配给不同的server.另外一点是region services用的是gossip protocol，每个server都知道所有其他的server负责哪些region，这样的话我们add/remove server就很方便，新加入的节点在瞬间能被其他server知道他负责哪个region。 首先乘客把需要多少个座位+++什么车型+++位置，发送给demand server,然后再次转发给supply server, 他知道乘客所在位置的block id,根据id找到对应的 region server，然后这个server画一个圈找到对应位置周围的出租车，找到他们的block ids, 并且把请求通过RPC call的方式转发给那些对应的region servers,每个region server调用 ETAservers计算自己的ETA，然后发送给客户端。把他们收集上来之后发送给ETA server去计算他们之间的距离,完毕之后把所有的结果发送给supply server, 那么当出租车司机点击抢单，就可以拿到对应的乘客。


4.GEO spatial: 首先使用S2 lib对区域进行划分，使得每个乘客都可以得到最近最优的出租车。并且同时使用google map帮助计算ETA,同时根据GPS location建立road network system。最后一点是preferred pick up point, 是用ML进行学习每次Uber司机都喜欢停在哪些位置。

5.ETA计算：首先会计算rider附近所有的出租车，但是有的时候有一个更近的出租车但是马上就完成了，所以UBER会计算所有的因素来计算最优解ETA。

6.Analytics和DB:UBER会把历史数据存储到RDMS中，同时实时数据可以存储到KAFKA,可以传递给HDFS做离线计算，可以传递给storm做实时计算，同时ETA server可以利用历史数据和实时数据对map进行更新，还可以计算traffic information, cab speeds.

UBER架构![image](https://user-images.githubusercontent.com/60555283/120643642-6a967e80-c444-11eb-880f-edb592a98259.png)



DDIA

=====================Single leader replications ==========================
1.写入操作必须通过leader, 然后leader把写入结果传递给所有的follower
2. 读取操作可以通过任意节点，leader或者follower都可以

同步和非同步的follower：
同步的follower: leader发送给follower,需要等待follower的返回才能执行下一步操作
异步的follower：leader发送给follower，之后可以做任何的事情，不需要等待。
通常集群中只有一个同步的follower，为的是怕leader挂掉，follower可以重新担当leader的职责，其他的节点都是异步的follower，为的是可以提升性能。

新加入的节点追赶
首先对leader的数据进行snapshot，然后把snapshot传递给follower，完毕之后follower问leader索要这段时间内产生变化的数据


处理挂机问题：
如果follower挂掉了，重启之后他会查看自己的log处理到哪一步了，然后向leader索要挂机这段时间内产生的数据变化
如果leader挂机了，需要三步，第一步是集群通过timeout来确定leader挂机了，如果follower一段时间内没收到leader的message,那么证明leader挂机。第二步，选举新的leader，一般选取最up-to-date的follower 第三步，client需要把写操作传递给新的leader，一旦old leader恢复了，一定要确认它变为follower。

Replication Logs的实现方式
1.statement-based replication
log记录的是数据库中每条statement(insert,delete,select),但是问题在于，某些statement是non-deterministic的，在不同的时间，或者不同的情况下执行的结果是不一样的，例如 now()或者自增函数，所以单纯的记录statement是行不通的
2.WAL
WAL是很底层的log，描述的是硬盘中哪个block的哪个byte被修改了，如果leader可以把WAL传递给follower，那么follower可以获取相同的数据。问题在于，假如follower和leader使用的storage version不一样，那么对于相同的log会产生不同的结果。如果follower的版本more newer的话，那么可以使用failover的办法重新选举，让这个follower作为新的leader，
3.Logic replication
和WAL不同的是，可以对storage version进行解耦，对于insert,delete,updated有着业务逻辑上的描述。这就允许follower和leader有着不同的storage engine version。

Leader-based replication的问题
replication的作用：可以使用更多的节点处理更多的请求++让一些replicas更接近users++fault tolerance
问题：有一些异步的follower会处于outdated状态，读取这些follower会产生过期数据，另外如果是同步的follower，那么集群如果很大，等待所有follower进行同步非常耗时
解决方案：
1.read-your-write consistency: 如果读取的是用户自己修改的数据，那么就从leader读。如果application大部分都是用户的修改的数据，可以设置时间限制，超过一定的时间会产生lagging，所以就从leader读取，如果replicas不够up-to-date,则从其他follower读取或者等待catch-up之后再读取。
2.时间一致性->单调读consistency: 如果client从replica的位置读取了两次，第一次读的是稍微过期的数据，第二次读的是过期很大的数据，则会产生先看到有评论，第二次再读取发现没有评论。单调读确保这种事情绝不会发生。解决方案就是最好都从一个replica读取。
3.因果一致性，如果Write A写入原因，write B写入结果，如果一个client读取的时候A产生了一些lagging，导致client先看到结果后看到原因。因果一致性确保了 如果写入的时候有顺序，那么读取的时候一定要按照写入的顺序来读取。

最期待的结果是developer不需要担心一致性，一致性应该由数据库来实现，对于单节点系统来说，使用事务transaction是最好的方案，但是性能有问题，所以大家摒弃了single-leader 


========================multiple  leader replications =============================
multi-leaders replications的应用场景：
1.如果集群需要跨越多个data centers的话，应该在每一个data center都设置一个leader,这样可以提升性能++容错++减少网络传输
2.对于offline application, 例如日历，当你在offline的时候操作，当恢复online的时候，需要和services进行同步处理, 你的每一台设备相当于一个leader，那么你的多个设备就像一个multi-leader的data center一样，他们之间需要互相同步
3.对于共同编辑文档的应用，通常情况下来说可以堪称single-leader模式，一个用户在编辑，另外一个用户必须等候解锁，但是为了加速操作，必须允许多个用户共同操作，这就是multi-leaders的模式


multi-leaders处理写入冲突：
在多人共同编辑文档应用中, 如果是single-leader replication,那么第一个writer写入的时候，第二个writer应该被block住，直到第一个写完之后它才能进行写入。
但是在multi-leaders中, 两个writers可以同时写入自己datacenter中的leader，然后在两个leaders进行同步的时候，发现conflict.
解决方案：
1.修改同一个field都转移到相同的leader上
2.给每一个写入操作赋予一个ID，ID大的覆盖掉ID小的写入操作
3.关于multi-leaders的paper，``Conflict-free replicated datatypes (CRDTs)`` ``Mergeable persistent data structure`` ``Operational transformation ``


Multi-leaders replication Topology:
因为多个leaders之间要进行同步策略,那么leaders之间的数据流向是怎么样的呢？
1.Circular
2.Star
3.All-to-all
对于前两种的问题在于，fault-tolerance处理的不好，一个节点挂掉了，其他的节点无法传递信息。每个node都有自己的ID，每次message被一个node接收之后，这条消息就要加上NODEID，直到消息上所有的NODEID都被标注之后，才证明传递完毕
All-to-all的问题在于，某个node收到其余两个node传递来的消息顺序和真实的逻辑顺序不一致，例如先收到结果，再收到原因，这会违反 因果一致性。



=============================leaderless replications ==================================
1.Quorum
对于没有leader的cluster,所有写入操作都要经过所有的nodes,如果收到 (N/2)+1 = w 以上node的回复，就算是写入成功
所有读取操作都要经过所有nodes, 读要读取r个节点, 因为这R个节点中肯定有确保里面有up-to-date的node
N=all nodes, w=r=(N+1)/2(round up), N一般是奇数, 但是如果是读多写少的系统，可以设置w=n,r=1即可

把w,r设置的比较小，虽然会读取stale data, 但是在很多机器挂掉不能使用的情况下，可以提供HA+lower latency

如果w,r 满足了w+r>n, 这种情况下，仍然会导致读到stale data.
1.Sloppy quorum: r,w不再有overlap
2. 多个读写并发产生，如果last winner wins, writes会丢失。
3. 读和写并发产生， 写入有可能没有写完w个节点,读取就开始发生了,所以会读取stale data.
4. 如果读取发生在w个节点上，但是不是所有的节点都写入成功，那么后续的read操作有可能读到最新数据，也可能读不到最新数据
虽然Quorum理论上读取可以读到最新数据，但是实际上会有很多问题。

对于leader-based replication来说，可以很好的monitoring，因为只要计算出当前stale日志的ID和leader日志的ID之间的差距，即可知道落后了多少。


2.Sloppy Quorums and Hinted Handoff
因为每次读写只需要等待w/r个节点, 所以Quorums有一定的容错性，但是网络出现问题，client无法连接w/r个节点，于是无法凑成w+r>n的条件。
这个时候如果开启了sloppy quorums功能可以暂时使用n之外的节点暂时接受读写请求，等到网络恢复再把读写请求转为正常的n个节点(hinted handoff)
所以sloppy quorums仍然要满足w+r>n.


3.处理并发写入
leaderless replication允许同一个client对同一个key进行多次写入，但是不同的node看到的结果顺序很可能不同，节点1看到先A后B，节点2看到先B后A，所以会出现不一致现象。
解决方案：
3.1 last write wins(LWW)
对每个写入操作都加入一个timestamp, 不同的node只以最后一个timestamp为主, 会把前面的数据给覆盖掉，保证数据一致性。Cassandra就是这么做的，但是问题在于，假如我们想保留之前的数据，LWW会把之前的数据覆盖。解决方案是可以使用UUID来做。

4.怎么定义并发写入？？
和时间没有关系，只要两个写入操作有逻辑上的依赖(A:insert where x=1 B:update x to 1)，就必然不是并发写入.
如果没有逻辑上的依赖，可以定义为并发。
1. Client 1 adds milk to the cart. This is the first write to that key, so the server suc‐ cessfully stores it and assigns it version 1. The server also echoes the value back to the client, along with the version number.
2. Client 2 adds eggs to the cart, not knowing that client 1 concurrently added milk (client 2 thought that its eggs were the only item in the cart). The server assigns version 2 to this write, and stores eggs and milk as two separate values. It then returns both values to the client, along with the version number of 2.
3. Client 1, oblivious to client 2’s write, wants to add flour to the cart, so it thinks the current cart contents should be [milk, flour]. It sends this value to the server, along with the version number 1 that the server gave client 1 previously. The server can tell from the version number that the write of [milk, flour] supersedes the prior value of [milk] but that it is concurrent with [eggs]. Thus, the server assigns version 3 to [milk, flour], overwrites the version 1 value [milk], but keeps the version 2 value [eggs] and returns both remaining values to the client.
4. Meanwhile, client 2 wants to add ham to the cart, unaware that client 1 just added flour. Client 2 received the two values [milk] and [eggs] from the server in the last response, so the client now merges those values and adds ham to form a new value, [eggs, milk, ham]. It sends that value to the server, along with the previ‐ ous version number 2. The server detects that version 2 overwrites [eggs] but is concurrent with [milk, flour], so the two remaining values are [milk, flour] with version 3, and [eggs, milk, ham] with version 4.
5. Finally, client 1 wants to add bacon. It previously received [milk, flour] and [eggs] from the server at version 3, so it merges those, adds bacon, and sends the final value [milk, flour, eggs, bacon] to the server, along with the version number 3. This overwrites [milk, flour] (note that [eggs] was already over‐ written in the last step) but is concurrent with [eggs, milk, ham], so the server keeps those two concurrent values.

<img width="1065" alt="截屏2021-05-30 下午12 24 02" src="https://user-images.githubusercontent.com/60555283/120111979-f1e0ab00-c141-11eb-8a89-ba2215250104.png">



4.1 The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written.
4.2 When a client reads a key, the server returns all values that have not been over‐ written, as well as the latest version number. A client must read a key before writing.
4.3 When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. (The response from a write request can be like a read, returning all current values, which allows us to chain several writes like in the shopping cart example.)
4.4 When the server receives a write with a particular version number, it can over‐ write all values with that version number or below (since it knows that they have been merged into the new value), but it must keep all values with a higher ver‐ sion number (because those values are concurrent with the incoming write).

最后三种一致性model

1.Read-after-write consistency
Users should always see data that they submitted themselves.
2. Monotonic reads
After users have seen the data at one point in time, they shouldn’t later see the data from some earlier point in time.
3. Consistent prefix reads
Users should see the data in a state that makes causal sense: for example, seeing a question and its reply in the correct order.



==========================PARTITION====================


===================================Fault tolerance===================================
要实现Fault tolerance必须实现一种抽象，例如在事务性数据库中，事务可以保证实现ACID，application开发者可以不用考虑这些问题。
对于Fault tolerance的抽象就是共识，比如leader挂了，所有的follower都达成一种共识，也就是需要重新选举新的节点。
所以我们的重点就是讨论如何实现共识机制。

strong consistency通常以牺牲性能和fault-tolerance为代价。
consistency的级别经常和transaction isolation的level相对应。但是事务主要关注的是并发操作中有没有race condition，而consistency关注的是协调状态的一致性。

1.linearizability(strong,atomic,external).
一个用户写完之后，任何的读取都必须要读取到最新鲜的数据，不能有任何stale data产生。虽然分布式系统中有很多节点，但是任何读写都要表现的像single node一样，任何操作都是atomic的

截屏2021-05-30 下午6.41.22![image](https://user-images.githubusercontent.com/60555283/120122438-a8f71980-c176-11eb-9584-7f7f5e34e736.png)

写入操作什么时候发生无所谓，但是一旦有一个read操作读取到了最新的数据，那么后续的 read操作一定不能读取到stale data.

2.Linearizaibility和 Serializability的区别：
后者是ACID 事务性数据库中保证的内容，并发执行的事务一定要表现的像一个有顺序的transaction execution order一样，即使这个order和真实的执行顺序不一样
前者在乎的是数据的新鲜度，每次写入完成之后，后续的读取操作一定要读到最新鲜的数据。
数据库中经常实现Linearizaibility+Serializability， 办法是2PL or  actual serial execution.
但是snapshot isolation并不是linearizable


3.Linearizability的使用场景
3.1 Lock和leader选举，通常情况下，ZooKeeper/etcd 这种协调性的服务都是使用分布式锁来实现Linearizability，一旦一个node获取了lock，另外其他的所有节点都必须用共识的办法承认这个node获取了leader。
3.2 唯一性的保证，比如创建linux path 或者 邮箱地址，一个用户一旦获取了锁创建了这个名字，其他用户不能再次创建相同的内容。
3.3.跨通道的时间保证，web server会先把图片存储到file storage中，然后通过MQ向file storage发送 resize的指令，假如指令发送的太快，比file storage中图片产生的变化更快，那么用户会看到stale data.所以这里需要Linearizability。


截屏2021-05-30 下午6.54.46![image](https://user-images.githubusercontent.com/60555283/120122736-9d0c5700-c178-11eb-8dcd-8364ad27637c.png)

4.实现Linearizability
实现fault-tolerance最好的办法是通过replication
4.1 对于single-leader replication：
通过读取leader或者sync的follower，一般来说会达成linearizability,但是如果leader是假的(拜占庭问题),那么就会破坏 linearizability。
共识算法解决了stale data和脑裂现象，所以共识算法会实现linearizability.

4.2 对于multiple-leader replication：
无法保证Linearizability

4.3 对于leaderless replication:
quorums并不一定会实现Linearizability，因为Last write wins是基于时钟系统，时钟系统无法正保证是真是的ordering。

截屏2021-05-30 下午7.12.36![image](https://user-images.githubusercontent.com/60555283/120123116-04c3a180-c17b-11eb-839c-e218998b5c00.png)



B的读取发生在A写入之后，但是无法保证读取到新鲜数据，因为B读取两个replica并没有被A写入完成。所以我们说leaderless replication就算实现了quorum，一样无法达成Linearizability.
除非：a reader must perform read repair synchronously, before returning results to the application。 
and a writer must read the latest state of a quorum of nodes before sending its writes。


5.cost of Linearizability
在multi-leaders replications的情况下，不同leader所领导的data center之间出现了网络问题，互相无法通信。此时此刻，系统应该允许读写操作可以继续呢(保证可用性，但是会读取到过期数据)？还是不让读写操作继续(保证Linearizability，但是无法保证可用性)？
这就是CAP理论所涉及的问题，当网络很好的情况下，我们可以既选择HA又选择consistency，但是网络不好的情况下必须二者选其一。也就是CAP理论，要不就是CP or AP.
当代计算机系统中的多核CPU也不是Linearizability的，每一个core都有自己的buffer和cache,计算机这么设计是为了达成性能上的优势。


1.因果一致性
因果一致性是partial order, Linearizability是total order。也就是说，在Linearizability中的任何的操作都能拿到对应的顺序，但是因果一致性只是保证业务逻辑上的前后顺序，假如不存在数据依赖关系，那么什么顺序都无所谓。
在强一致性的系统中，没有并发操作。对于所有的operations一定有唯一的顺序保证。
大部分的数据系统都使用因果一致性而不是强一致性，因为考虑到性能问题。强一致性一般也就顺便实现了因果一致性
snapshot isolation 提供的就是因果一致性


在实际工作中，追求因果关系很不现实，因为application在写入之前通常需要读取大量的数据，很难确定write operation取决于哪个read读取。
所以，我们可以使用sequence number 或者 timestamp(come from logical clock)，他们一般来说都是total order的, 每个操作都有唯一的ID，任意两个操作都可以被对比。
consistent with causality: 如果A happens before B，那么A sequence number一定在B之前。如果是并发的操作，无法保证sequence number是否递增以及有顺序。
在single-leader replication中, replication logs就定义了total order的写入操作并且consistent with causality. leader可以对每一个operation都设置一个递增的ID，如果follower使用这些ID就可以满足因果一致性。   
如果是multiple-leaders, 产生sequence number的办法是，(一个节点产生偶数，一个节点产生奇数)+++(每个操作附加一个timestamp)+++(每个节点提前allocation一定量的ID)
虽然这三种办法可以实现sequence number 但是无法consistent with causality，也就是说虽然是递增的ID，但是并不一定能保证业务逻辑上的顺序准确。

所以使用Lamport timestamp(LT), 可以保证了consistent with causality. The key idea about Lamport timestamps, which makes them consis‐ tent with causality, is the following: every node and every client keeps track of the maximum counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.

仅仅只有完美的timestamp 或者sequence number还不够， 时间戳只有把所有操作全部收集之后，才能有一个顺序。但是某个节点接受了创建文件的请求，他必须保证其他节点没有正在创建相同的文件名字并且以一个lower 时间戳在创建。所以必须去每个节点上去check。

还需要total order broadcasting（TOB）:  每一个节点处理相同的操作的时候，必须保证这个操作在所有节点上的顺序保持一致。TOB可以用来实现serializable transactions。
因为TOB不允许后来的消息被提前写入，所以保证了TOP比timestamp ordering要更强。
TOB可以用来创建LOG，类似于replication log,transaction log,write-ahead log,delivering a message is like appending to the log. Since all nodes must deliver the same messages in the same order, all nodes can read the log and see the same sequence of messages
TOB可以用来做lock service，Every request to acquire the lock is appended as a message to the log, and all messages are sequentially numbered in the order they appear in the log

使用TOB来实现 线性读和线性写
TOB并不是线性的，因为它只是确保了每个节点上的消息顺序保持完全一致。但是无法保证正确的顺序，而线性就能保证读一定会读取到最新鲜的数据。
线性写入：
假如很多用户都想提交同一个username, 向leader append log，然后读取log files，如果发现第一个log就是自己本身发送出去的，这就证明自己的username的已经成功了，如果不是，那就证明自己的username添加失败。因为TOB保证了所有的节点观测到的log顺序保持完全一致，所以这就保证了线性写入。只有成功的那个用户会写入username，其他用户会直接打断自己的写入。
线性读取：
以上可以保证线性写入，但不能保证线性读取，虽然TOB保证了每台机器上的log的存储顺序是一致的，但是如果有async的write操作，也就是虽然write的log顺序靠前，但是实际上write发生的时间比read晚，这就导致了stale读取。
解决方案
1.你可以先添加message,再read log,当message 返回你的时候最后再perform真实的read, 当message返回你的时候write写完了。
2.如果log可以允许读取latest log messages,  那么就是线性读取
3.读sync的write操作，不允许有async出现。

使用线性存储来实现TOB
首先先实现atomic increment-and-get operation/compare-and-set operation, 对于每一条消息都把用atomic产生的sequence number附加上去，然后，您可以将messages发送到所有节点（重新发送丢失的messeages），收件人将按序列号连续发送messages。Note that unlike Lamport timestamps, the numbers you get from incrementing the linearizable register form a sequence with no gaps. Thus, if a node has delivered mes‐ sage 4 and receives an incoming message with a sequence number of 6, it knows that it must wait for message 5 before it can deliver message 6. The same is not the case—in fact, this is the key difference between total order broadcast and timestamp ordering.



4.分布式的事务和共识
需要产生共识的场景.
4.1 leader election: 一旦新的节点产生，每个节点都要承认这个新节点就是leader。不然会产生脑裂现象。
4.2 atomic commit：一个事务有可能在某些节点失败，在某些节点成功，所有的节点都需要达成共识，来确定哪个节点是成功的，哪个节点是失败的

2PC
原子性提交的目的，就是使得事务不然就是全部提交成功，不然就是全部提交失败，不能有中间的状态。
single-node atomic commit:
对于单节点来说，会首先写入一条日志，然后再写入业务数据，一旦日志数据写入完成，就代表commit成功。就算业务数据没有被写入的情况下挂机了，那么日志文件仍然会在重启的时候帮助恢复业务数据。
multiple-node atomic commit:
需要用到2PC。2PC总的来说需要coordinator+participants, applicaiton首先需要在每个participants上完成写入和read操作。一旦中间有错误发生，coordinator可以abort,一旦完成之后coordinator可以发送prepare指令给所有的participants，每个participants查看是否可以提交，如果能，就返回yes给coordinator，如果收到所有的回复都是yes，那么coordinator首先把决策结果写入日志，完成之后就发送commit message给所有的participants进行提交，
Coordinator有可能在发送commit message的过程中挂掉了,一些node收到commit指令，一些没有收到，这些收到的必须无限等待下去，直到coordinator恢复。这就是为什么coordinator一定要在发送commit message之前把log写入disk.


大多数的分布式系统使用2PC实现分布式事务。很多情况下我们需要在不同的大数据系统之间实现2PC，这就需要XA transaction, 实现XA transaction的coordinator可以在多个不同的大数据系统之间实现2PC。
2PC中的任何一个participant在等待coordinator的过程中都不能做任何事情，因为这个时候row-level有锁，就算 coordinator一天不回应，participants也得一天不做任何操作。

分布式事务出现的问题：
1. If the coordinator is not replicated but runs only on a single machine, it is a sin‐ gle point of failure for the entire system (since its failure causes other application servers to block on locks held by in-doubt transactions). Surprisingly, many coordinator implementations are not highly available by default, or have only rudimentary replication support.
2. Since XA needs to be compatible with a wide range of data systems, it is necessar‐ ily a lowest common denominator. For example, it cannot detect deadlocks across different systems (since that would require a standardized protocol for systems to exchange information on the locks that each transaction is waiting for), and it does not work with SSI (see “Serializable Snapshot Isolation (SSI)” on page 261), since that would require a protocol for identifying conflicts across dif‐ ferent systems.
3. For database-internal distributed transactions (not XA), the limitations are not so great—for example, a distributed version of SSI is possible. However, there remains the problem that for 2PC to successfully commit a transaction, all par‐ ticipants must respond. Consequently, if any part of the system is broken, the transaction also fails. Distributed transactions thus have a tendency of amplifying failures, which runs counter to our goal of building fault-tolerant systems.


共识算法：
每个节点都会propose不同的数据，那么共识算法的目的就是让他们都达成同样的数据结果。
通常有以下4个不同的特性
1.uniform agreement:No two nodes decide differently.
2.Integrity:No node decides twice.
3.Validity:If a node decides value v, then v was proposed by some node.
4.Termination: Every node that does not crash eventually decides some value.

The uniform agreement and integrity properties define the core idea of consensus: everyone decides on the same outcome, and once you have decided, you cannot change your mind. The validity property exists mostly to rule out trivial solutions: for example, you could have an algorithm that always decides null, no matter what was proposed; this algorithm would satisfy the agreement and integrity properties, but not the validity property.

If you don’t care about fault tolerance, then satisfying the first three properties is easy: you can just hardcode one node to be the “dictator,” and let that node make all of the decisions. However, if that one node fails, then the system can no longer make any decisions. This is, in fact, what we saw in the case of two-phase commit: if the coordinator fails, in-doubt participants cannot decide whether to commit or abort.


共识算法包括：VSR,Paxos,Raft and Zab.

Remember that total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes. If you think about it, this is equivalent to performing several rounds of consensus: in each round, nodes propose the message that they want to send next, and then decide on the next message to be delivered in the total order [67].
So, total order broadcast is equivalent to repeated rounds of consensus (each consen‐ sus decision corresponding to one message delivery):
• Due to the agreement property of consensus, all nodes decide to deliver the same messages in the same order.
• Due to the integrity property, messages are not duplicated.
• Due to the validity property, messages are not corrupted and not fabricated out of thin air.
• Due to the termination property, messages are not lost.
Viewstamped Replication, Raft, and Zab implement total order broadcast directly, because that is more efficient than doing repeated rounds of one-value-at-a-time consensus. In the case of Paxos, this optimization is known as Multi-Paxos.

DDIA pages 389.


===================================一致性哈希=========================================
1.哈希的意思是只要输出一样，那么输入一定一样。
2.负载均衡是指多个服务器中，分配到每个服务器上的请求数量基本差不多。
如果使用传统的办法，先对server ID做一次哈希，然后把结果在用mode计算，那么虽然目前来说没问题，但是如果cluster size变化了，那么mode的参数也要随之变化，所有的之前发送的request的server也要变化。
3.所以如果把所有服务器放在一个环上，每个服务器只接收上游的请求，这样就算添加新的server,只会影响部分。


一致性哈希![image](https://user-images.githubusercontent.com/60555283/120567973-f24aa180-c3e0-11eb-9bea-1e2c3bf77c56.png)


























