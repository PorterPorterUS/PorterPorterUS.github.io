---
layout: post
author: Xusheng Ji
title: "DDIA"
tags: System Design
categories: Database System
---

{% include lib/mathjax.html %}


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>



DDIA=
=====================Single leader replications ==========================
1.写入操作必须通过leader, 然后leader把写入结果传递给所有的follower
2. 读取操作可以通过任意节点，leader或者follower都可以

同步和非同步的follower：
同步的follower: leader发送给follower,需要等待follower的返回才能执行下一步操作
异步的follower：leader发送给follower，之后可以做任何的事情，不需要等待。
通常集群中只有一个同步的follower，为的是怕leader挂掉，follower可以重新担当leader的职责，其他的节点都是异步的follower，为的是可以提升性能。

新加入的节点追赶
首先对leader的数据进行snapshot，然后把snapshot传递给follower，完毕之后follower问leader索要这段时间内产生变化的数据


处理挂机问题：
如果follower挂掉了，重启之后他会查看自己的log处理到哪一步了，然后向leader索要挂机这段时间内产生的数据变化
如果leader挂机了，需要三步，第一步是集群通过timeout来确定leader挂机了，如果follower一段时间内没收到leader的message,那么证明leader挂机。第二步，选举新的leader，一般选取最up-to-date的follower 第三步，client需要把写操作传递给新的leader，一旦old leader恢复了，一定要确认它变为follower。

Replication Logs的实现方式
1.statement-based replication
log记录的是数据库中每条statement(insert,delete,select),但是问题在于，某些statement是non-deterministic的，在不同的时间，或者不同的情况下执行的结果是不一样的，例如 now()或者自增函数，所以单纯的记录statement是行不通的
2.WAL
WAL是很底层的log，描述的是硬盘中哪个block的哪个byte被修改了，如果leader可以把WAL传递给follower，那么follower可以获取相同的数据。问题在于，假如follower和leader使用的storage version不一样，那么对于相同的log会产生不同的结果。如果follower的版本more newer的话，那么可以使用failover的办法重新选举，让这个follower作为新的leader，
3.Logic replication
和WAL不同的是，可以对storage version进行解耦，对于insert,delete,updated有着业务逻辑上的描述。这就允许follower和leader有着不同的storage engine version。

Leader-based replication的问题
replication的作用：可以使用更多的节点处理更多的请求++让一些replicas更接近users++fault tolerance
问题：有一些异步的follower会处于outdated状态，读取这些follower会产生过期数据，另外如果是同步的follower，那么集群如果很大，等待所有follower进行同步非常耗时
解决方案：
1.read-your-write consistency: 如果读取的是用户自己修改的数据，那么就从leader读。如果application大部分都是用户的修改的数据，可以设置时间限制，超过一定的时间会产生lagging，所以就从leader读取，如果replicas不够up-to-date,则从其他follower读取或者等待catch-up之后再读取。
2.时间一致性->单调读consistency: 如果client从replica的位置读取了两次，第一次读的是稍微过期的数据，第二次读的是过期很大的数据，则会产生先看到有评论，第二次再读取发现没有评论。单调读确保这种事情绝不会发生。解决方案就是最好都从一个replica读取。
3.因果一致性，如果Write A写入原因，write B写入结果，如果一个client读取的时候A产生了一些lagging，导致client先看到结果后看到原因。因果一致性确保了 如果写入的时候有顺序，那么读取的时候一定要按照写入的顺序来读取。

最期待的结果是developer不需要担心一致性，一致性应该由数据库来实现，对于单节点系统来说，使用事务transaction是最好的方案，但是性能有问题，所以大家摒弃了single-leader 


========================multiple  leader replications =============================
multi-leaders replications的应用场景：
1.如果集群需要跨越多个data centers的话，应该在每一个data center都设置一个leader,这样可以提升性能++容错++减少网络传输
2.对于offline application, 例如日历，当你在offline的时候操作，当恢复online的时候，需要和services进行同步处理, 你的每一台设备相当于一个leader，那么你的多个设备就像一个multi-leader的data center一样，他们之间需要互相同步
3.对于共同编辑文档的应用，通常情况下来说可以堪称single-leader模式，一个用户在编辑，另外一个用户必须等候解锁，但是为了加速操作，必须允许多个用户共同操作，这就是multi-leaders的模式


multi-leaders处理写入冲突：
在多人共同编辑文档应用中, 如果是single-leader replication,那么第一个writer写入的时候，第二个writer应该被block住，直到第一个写完之后它才能进行写入。
但是在multi-leaders中, 两个writers可以同时写入自己datacenter中的leader，然后在两个leaders进行同步的时候，发现conflict.
解决方案：
1.修改同一个field都转移到相同的leader上
2.给每一个写入操作赋予一个ID，ID大的覆盖掉ID小的写入操作
3.关于multi-leaders的paper，``Conflict-free replicated datatypes (CRDTs)`` ``Mergeable persistent data structure`` ``Operational transformation ``


Multi-leaders replication Topology:
因为多个leaders之间要进行同步策略,那么leaders之间的数据流向是怎么样的呢？
1.Circular
2.Star
3.All-to-all
对于前两种的问题在于，fault-tolerance处理的不好，一个节点挂掉了，其他的节点无法传递信息。每个node都有自己的ID，每次message被一个node接收之后，这条消息就要加上NODEID，直到消息上所有的NODEID都被标注之后，才证明传递完毕
All-to-all的问题在于，某个node收到其余两个node传递来的消息顺序和真实的逻辑顺序不一致，例如先收到结果，再收到原因，这会违反 因果一致性。



=========================leaderless replications ==========================
对于没有leader的cluster,所有写入操作都要经过所有的nodes,如果收到 (N/2)+1 = w 以上node的回复，就算是写入成功
读要读取r个节点, 因为这R个节点中肯定有确保里面有up-to-date的node

正式的来讲，如果我们有N replicas, 每次写入操作有w个节点确认写入成功，就算成功；每次读取操作，必须每次确认最少读取r个节点. 并且满足 W+R>N
通常来说N是个奇数, w=r=(n+1)/2(round up), 但是如果是读多写少的系统，可以设置w=n,r=1即可

通常来说，read/write都要经过所有的节点, 如果有w个节点确认写入成功, R个节点确认读取成功，那么就能保证数据新鲜。






















